{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>Assignment 4 [Final]","metadata":{"id":"xksEYwr8QXgN"}},{"cell_type":"code","source":"Your_name = \"Ishan Pandey\"\nYour_emailid = \"ishanpandeycse@gmail.com\"","metadata":{"id":"2WVqrOVwm8yz","execution":{"iopub.status.busy":"2022-08-21T00:13:24.259626Z","iopub.execute_input":"2022-08-21T00:13:24.261175Z","iopub.status.idle":"2022-08-21T00:13:24.285570Z","shell.execute_reply.started":"2022-08-21T00:13:24.260478Z","shell.execute_reply":"2022-08-21T00:13:24.284467Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### **NOTE: Open this notebook in kaggle and import Artificial Lunar Landscape Dataset.** ","metadata":{"id":"_qWKqzF4415S"}},{"cell_type":"markdown","source":"## > If you run this notebook as it is, you will get the val_iou_score of around 0.20 (remember to use GPU for training the model)\n\n## > Your goal is to increase the val_iou_score as much as you can for this project using any method. The evaluation of this assignment will be based on your acquired val_iou_score. One point for each increasing 0.01 val_iou_score. \n\n> For example - if val_iou_score = 0.41, your points will be 41/100. \n\n## > Please check your notebook before submission, try to avoid any error.\n\n### Some tips to increase the performance\n* Increase the number of epochs\n* Increase the number of layers in your model\n* Using SOTA high performance networks with transfer learning\n* Using callbacks and carefully observing your model performance\n\nYou are free to use other techniques too. ","metadata":{"id":"rb3HM3Asm8y1"}},{"cell_type":"markdown","source":"# GUIDELINES ABOUT MAKING CHANGES TO THIS NOTEBOOK\n\nFor every change you make to this notebook, only those supported by the following would be considered for evaluation:\n\n1. A descriptive comment explaining the change, e.g., if you are adding an extra conv2d layer, write about all the aspects of the conv2d layer you are adding. The comment should be placed at the point where the layer will be added. \n\n2. Changes brought to the system because of changes you introduced, e.g., if you changed layers - added, deleted, etc., you MUST show model properties before and after the changes were made.\n\n3. Data preprocessing changes - if you use new data preprocessing techniques that are not a part of this notebook, you MUST explain their inner workings using 2-3 MARKDOWN cells, and ONLY AFTER THAT,  proceed to use that technique. Without this explanation, your technique will not be considered for evaluation.\n\n4. ALL improvements MUST BE REPORTED VIA PLOTS OR TABLES OR BOTH, e.g., if increasing epochs from 30 to 50 improved your results, but decreasing learning_rate from 0.0001 to 0.00005 also improved your results, then these gains FIRST HAVE TO BE REPORTED SEPARATELY VIA PLOTS, THEN AGAIN TOGETHER VIA TABLES. \n\n  One plot can show iou values increasing from epoch 30 to epoch 50. Another plot can show iou values improving at varying learning rates. Finally tables can be used to show iou values for different learning rates, table 1 for lr_1 shows iou for epochs 30 through 50, table 2 for lr_2 shows iou for epochs 30 through 50, and so on and so forth - ALL YOUR RESULTS MUST BE COMPULSORILY QUANTIFIABLE! \n\nIt is therefore advised to work on one improvement, optimize it, plot it, document it, then proceed to the next improvement - till you get a satisfactory IOU score.\n\n5. FINAL IMPROVEMENT SUMMARY TABLE: Prepare a table with columns (improv#, description, increase in iou from, increase in iou to), and list out all the improvements you made to improve your model performance.\n","metadata":{"id":"CSjKJ2JQtLV9"}},{"cell_type":"code","source":"!pip install segmentation_models","metadata":{"id":"HRJUKjN1QXgS","execution":{"iopub.status.busy":"2022-08-21T00:13:24.287549Z","iopub.execute_input":"2022-08-21T00:13:24.287922Z","iopub.status.idle":"2022-08-21T00:13:36.080327Z","shell.execute_reply.started":"2022-08-21T00:13:24.287886Z","shell.execute_reply":"2022-08-21T00:13:36.079057Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# import the necessary Library\n\nimport tensorflow as tf\nimport segmentation_models as sm\nimport glob\nimport cv2\nimport os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport keras \nfrom sklearn.model_selection import train_test_split","metadata":{"id":"bGIcq3_DQXgT","execution":{"iopub.status.busy":"2022-08-21T00:13:36.083078Z","iopub.execute_input":"2022-08-21T00:13:36.083481Z","iopub.status.idle":"2022-08-21T00:13:41.767220Z","shell.execute_reply.started":"2022-08-21T00:13:36.083431Z","shell.execute_reply":"2022-08-21T00:13:41.766271Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"* Provide environment variable SM_FRAMEWORK=keras / SM_FRAMEWORK=tf.keras before import segmentation_models\n* Change framework sm.set_framework('keras') / sm.set_framework('tf.keras')","metadata":{"id":"TIJMgWvKm8y7"}},{"cell_type":"code","source":"# Setting framework environment\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\nsm.set_framework('tf.keras')\nkeras.backend.set_image_data_format('channels_last')","metadata":{"id":"x2HKIVofm8y7","execution":{"iopub.status.busy":"2022-08-21T00:13:41.769913Z","iopub.execute_input":"2022-08-21T00:13:41.770577Z","iopub.status.idle":"2022-08-21T00:13:42.132746Z","shell.execute_reply.started":"2022-08-21T00:13:41.770538Z","shell.execute_reply":"2022-08-21T00:13:42.131711Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing Pipeline","metadata":{"id":"OdS9NTtwQXgV"}},{"cell_type":"code","source":"H = 256 # height of image\nW = 256 # width of image\n\n'''This function is used to return the list of path for images and masks in\nsorted order from the given directory respectively.'''\n# function to return list of image paths and mask paths \ndef process_data(IMG_DIR, MASK_DIR):\n    images = [os.path.join(IMG_DIR, x) for x in sorted(os.listdir(IMG_DIR))]\n    masks = [os.path.join(MASK_DIR, x) for x in sorted(os.listdir(MASK_DIR))]\n\n    return images, masks\n\n'''This function is used to return splitted list of images and corresponding \nmask paths in train and test by providing test size.'''\n# function to load data and train test split\ndef load_data(IMG_DIR, MASK_DIR):\n    X, y = process_data(IMG_DIR, MASK_DIR)\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42)\n    \n    return X_train, X_test, y_train, y_test\n\n'''This function is used to read images. It takes image path as input. \nAfter reading image it is resized by width and height provide above(256 x 256). \nNext normalization is done by dividing each values with 255. And the result is returned.'''\n# function to read image\ndef read_image(x):\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    return x\n\n'''This function is used to read masks.'''\n# function to read mask\ndef read_mask(x):\n    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n  #change 1  \n    x= x // 0.07 \n    #I made the above change due to the fact that the image is a masked image and we have to do thresholding between 0s and 1s. This was also taught in the class itself.\n    #The next two lines below are the wrong classification as we had discussed in the class . Since it is same dataset, I corrected it.\n    x[x==3]=2 #new added due to wrong classification as mentioned in class\n    x[x==10] =3      #new added due to wrong classification as mentioned in class\n    \n    \n    x = x.astype(np.int32)\n    return x\n\n'''This function is used to generate tensorflow data pipeline. \nThe tensorflow data pipeline is mapped to function ‘preprocess’ .'''\n# function for tensorflow dataset pipeline\ndef tf_dataset(x, y, batch=8):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.shuffle(buffer_size=5000)\n    dataset = dataset.map(preprocess,tf.data.AUTOTUNE) #added autotune\n    #I just made this part a little more efficient by adding autotune but it did not play any part in increasing iou score\n    dataset = dataset.batch(batch)\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(2)\n    return dataset\n\n'''This function takes image and mask path. \nIt reads the image and mask as provided by paths. \nMask is one hot encoded for multi class segmentation (here 4 class).'''\n# function to read image and mask amd create one hot encoding for mask\ndef preprocess(x, y):\n    def f(x, y):\n        x = x.decode()\n        y = y.decode()\n\n        image = read_image(x)\n        mask = read_mask(y)\n\n        return image, mask\n\n    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.int32])\n    mask = tf.one_hot(mask, 4, dtype=tf.int32)\n    image.set_shape([H, W, 3])\n    mask.set_shape([H, W, 4])\n\n    return image, mask","metadata":{"id":"EQGsLbOVQXgW","execution":{"iopub.status.busy":"2022-08-21T00:13:42.134419Z","iopub.execute_input":"2022-08-21T00:13:42.134861Z","iopub.status.idle":"2022-08-21T00:13:42.154367Z","shell.execute_reply.started":"2022-08-21T00:13:42.134824Z","shell.execute_reply":"2022-08-21T00:13:42.151078Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataset","metadata":{"id":"ufOlyg7MQXgY"}},{"cell_type":"code","source":"'''RENDER_IMAGE_DIR_PATH: ‘Path of image directory’\nGROUND_MASK_DIR_PATH: ‘Path of mask directory’\n\nHere load_data function is called. This will load the dataset paths and \nsplit it into X_train, X_test, y_train, y_test '''\n\nRENDER_IMAGE_DIR_PATH = '../input/artificial-lunar-rocky-landscape-dataset/images/render'\nGROUND_MASK_DIR_PATH = '../input/artificial-lunar-rocky-landscape-dataset/images/clean'\n\nX_train, X_test, y_train, y_test = load_data(RENDER_IMAGE_DIR_PATH, GROUND_MASK_DIR_PATH)\nprint(f\"Dataset:\\n Train: {len(X_train)} \\n Test: {len(X_test)}\")","metadata":{"id":"vHWstFNTQXgY","execution":{"iopub.status.busy":"2022-08-21T00:13:42.155831Z","iopub.execute_input":"2022-08-21T00:13:42.156668Z","iopub.status.idle":"2022-08-21T00:13:42.921955Z","shell.execute_reply.started":"2022-08-21T00:13:42.156612Z","shell.execute_reply":"2022-08-21T00:13:42.920925Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Generate tensorflow data pipeline","metadata":{"id":"WfSTVsjCQXgZ"}},{"cell_type":"code","source":"batch_size = 8\n\n'''Here the tf_dataset function is called will generate the tensorflow data pipeline.'''\n# calling tf_dataset\ntrain_dataset = tf_dataset(X_train, y_train, batch=batch_size)\nvalid_dataset = tf_dataset(X_test, y_test, batch=batch_size)","metadata":{"id":"4xsJKtW0QXgZ","execution":{"iopub.status.busy":"2022-08-21T00:13:42.923566Z","iopub.execute_input":"2022-08-21T00:13:42.923935Z","iopub.status.idle":"2022-08-21T00:13:46.004938Z","shell.execute_reply.started":"2022-08-21T00:13:42.923899Z","shell.execute_reply":"2022-08-21T00:13:46.003943Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Creating U-net Architecture","metadata":{"id":"1MqxtDTmQXga"}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate\nfrom tensorflow.keras.models import Model\n\n# '''conv_block it is used to create one block with two convolution layer \n# followed by BatchNormalization and activation function relu. \n# If the pooling is required then Maxpool2D is applied and return it else not.'''\n# # function to create convolution block\n# def conv_block(inputs, filters, pool=True):\n#     x = Conv2D(filters, 3, padding=\"same\")(inputs)\n#     x = BatchNormalization()(x)\n#     x = Activation(\"relu\")(x)\n\n#     x = Conv2D(filters, 3, padding=\"same\")(x)\n#     x = BatchNormalization()(x)\n#     x = Activation(\"relu\")(x)\n\n#     if pool == True:\n#         p = MaxPool2D((2, 2))(x)\n#         return x, p\n#     else:\n#         return x\n\n# '''build_unet it is used to create the U-net architecture.'''\n# # function to build U-net\n# def build_unet(shape, num_classes):\n#     inputs = Input(shape)\n    \n#     #aim for using transfer learning\n    \n\n#     \"\"\" Encoder \"\"\"\n#     x1, p1 = conv_block(inputs, 16, pool=True)\n#     x2, p2 = conv_block(p1, 32, pool=True)\n#     x3, p3 = conv_block(p2, 48, pool=True)\n#     x4, p4 = conv_block(p3, 64, pool=True)\n\n#     \"\"\" Bridge \"\"\"\n#     b1 = conv_block(p4, 128, pool=False)\n\n#     \"\"\" Decoder \"\"\"\n#     u1 = UpSampling2D((2, 2), interpolation=\"bilinear\")(b1)\n#     c1 = Concatenate()([u1, x4])\n#     x5 = conv_block(c1, 64, pool=False)\n\n#     u2 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x5)\n#     c2 = Concatenate()([u2, x3])\n#     x6 = conv_block(c2, 48, pool=False)\n\n#     u3 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x6)\n#     c3 = Concatenate()([u3, x2])\n#     x7 = conv_block(c3, 32, pool=False)\n\n#     u4 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x7)\n#     c4 = Concatenate()([u4, x1])\n#     x8 = conv_block(c4, 16, pool=False)\n\n#     \"\"\" Output layer \"\"\"\n#     output = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(x8)\n\n#     return Model(inputs, output)\n\n\n\n#########################################################\nfrom keras.layers import Input, Conv2D\n\n\n\n\n\nBACKBONE = 'vgg16'\ninput_shape = (256, 256, 3)\nn_classes = 4\nactivation = 'softmax'\n\n# using segmentation_models to create U-net with vgg16 as a backbone\n# and pretrained imagenet weights\n#Similar problem was discussed in the class where we also had the same dataset . So my approach was just to use transferleaning SOTA models to solve the problem.\n\n\nmodel = sm.Unet(backbone_name = BACKBONE, \n                input_shape = input_shape, \n                classes = n_classes, \n                activation = activation,\n                encoder_weights = 'imagenet')\nmodel.summary()\n\n\n\n\n","metadata":{"id":"tYCyf8smQXga","execution":{"iopub.status.busy":"2022-08-21T00:13:46.006533Z","iopub.execute_input":"2022-08-21T00:13:46.007154Z","iopub.status.idle":"2022-08-21T00:13:48.615144Z","shell.execute_reply.started":"2022-08-21T00:13:46.007117Z","shell.execute_reply":"2022-08-21T00:13:48.614142Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# # calling build_unet function\n# model = build_unet((256, 256, 3), 4)\n\n# #printing model summary\n# model.summary()","metadata":{"id":"65hPnreJQXgb","execution":{"iopub.status.busy":"2022-08-21T00:13:48.618448Z","iopub.execute_input":"2022-08-21T00:13:48.618779Z","iopub.status.idle":"2022-08-21T00:13:48.623280Z","shell.execute_reply.started":"2022-08-21T00:13:48.618753Z","shell.execute_reply":"2022-08-21T00:13:48.621837Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Load model and compile","metadata":{"id":"bMgeqmX2QXgc"}},{"cell_type":"code","source":"# from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n# from segmentation_models.metrics import iou_score\n# import datetime, os\n\n# \"\"\" Defining Hyperparameters \"\"\"\n# img_shape = (256, 256, 3)\n# num_classes = 4\n# lr = 1e-4\n# batch_size = 16\n# epochs = 5\n\n# \"\"\" Model building and compiling \"\"\"\n# model = build_unet(img_shape, num_classes)\n# model.compile(loss=\"categorical_crossentropy\", \n#               optimizer=tf.keras.optimizers.Adam(lr), \n#               metrics=[iou_score])\n\n\n# train_steps = len(X_train)//batch_size\n# valid_steps = len(X_test)//batch_size\n\n# '''model.fit is used to train the model'''\n# model_history = model.fit(train_dataset,\n#         steps_per_epoch=train_steps,\n#         validation_data=valid_dataset,\n#         validation_steps=valid_steps,\n#         epochs=epochs,\n#     )\n#################################################################################\n\n# importing libraries\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom segmentation_models.metrics import iou_score\nimport datetime, os\n\n\"\"\" Hyperparameters \"\"\"\nlr = 1e-4\nbatch_size = 16\nepochs = 5\n\n# metrics for result validation\nmetrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n\n# compiling the model\nmodel.compile(loss = 'categorical_crossentropy', \n              optimizer = tf.keras.optimizers.Adam(lr), \n              metrics = metrics)\n\ntrain_steps = len(X_train)//batch_size\nvalid_steps = len(X_test)//batch_size\n\n\n#Change 2 - added callbacks\n#I have added callbacks but that was also to practice good programming practice but since my epoch is 5 , there is not much use of the \n#callbacks I have added , the major difference has come from using SOTA model only and changing the faults in dataset.\n\n\"\"\" Callbacks \"\"\"\ncurrent_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks\ncallbacks = [\n        tf.keras.callbacks.ModelCheckpoint(filepath=f'models/lunarModel_{current_datetime}.h5',\n                        monitor='val_iou_score', verbose=0, \n                        mode='max', save_best_model=False),\n             \n        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_iou_score\", mode='max', patience=4,\n                          factor=0.1, verbose=0, min_lr=1e-6),\n             \n        tf.keras.callbacks.EarlyStopping(monitor=\"val_iou_score\", patience=5, verbose=0, mode='max'),\n\n        tf.keras.callbacks.TensorBoard(f'models/logs_{current_datetime}')\n    ]","metadata":{"id":"z91qV2ZwQXgc","execution":{"iopub.status.busy":"2022-08-21T00:13:48.625809Z","iopub.execute_input":"2022-08-21T00:13:48.626238Z","iopub.status.idle":"2022-08-21T00:13:48.995701Z","shell.execute_reply.started":"2022-08-21T00:13:48.626171Z","shell.execute_reply":"2022-08-21T00:13:48.994571Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# History without transfer learning\nEpoch 1/5\n488/488 [==============================] - 69s 136ms/step - loss: 0.2894 - iou_score: 0.1600 - val_loss: 0.1511 - val_iou_score: 0.1680\nEpoch 2/5\n488/488 [==============================] - 66s 136ms/step - loss: 0.1362 - iou_score: 0.1888 - val_loss: 0.1036 - val_iou_score: 0.1941\nEpoch 3/5\n488/488 [==============================] - 67s 136ms/step - loss: 0.0912 - iou_score: 0.1967 - val_loss: 0.0822 - val_iou_score: 0.1980\nEpoch 4/5\n488/488 [==============================] - 67s 137ms/step - loss: 0.0670 - iou_score: 0.2029 - val_loss: 0.0520 - val_iou_score: 0.2060\nEpoch 5/5\n488/488 [==============================] - 67s 138ms/step - loss: 0.0501 - iou_score: 0.2063 - val_loss: 0.0449 - val_iou_score: 0.2081","metadata":{}},{"cell_type":"markdown","source":"## Train model","metadata":{"id":"SBhRPBKPQXgc"}},{"cell_type":"code","source":"'''model.fit is used to train the model'''\nmodel_history = model.fit(train_dataset,\n        steps_per_epoch=train_steps,\n        validation_data=valid_dataset,\n        validation_steps=valid_steps,\n        epochs=epochs,\n    )","metadata":{"id":"4lJgBNVwQXgd","execution":{"iopub.status.busy":"2022-08-21T00:13:48.996901Z","iopub.execute_input":"2022-08-21T00:13:48.998213Z","iopub.status.idle":"2022-08-21T00:16:15.256774Z","shell.execute_reply.started":"2022-08-21T00:13:48.998172Z","shell.execute_reply":"2022-08-21T00:16:15.253315Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## [IMPORTANT] Paste you final model training history here in the markdown.(just double click this line, and you'll be able to edit it. \n\nNOTE: If we find that your actual model score and what you paste here is differing, your assignment will get rejected.  \n\nhere ----\n2022-08-20 22:53:04.008158: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\nEpoch 1/5\n2022-08-20 22:53:07.562971: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n488/488 [==============================] - 117s 216ms/step - loss: 0.2117 - iou_score: 0.5409 - f1-score: 0.5551 - val_loss: 0.0709 - val_iou_score: 0.8074 - val_f1-score: 0.8251\nEpoch 2/5\n488/488 [==============================] - 101s 207ms/step - loss: 0.0461 - iou_score: 0.9487 - f1-score: 0.9585 - val_loss: 0.0414 - val_iou_score: 0.9540 - val_f1-score: 0.9746\nEpoch 3/5\n488/488 [==============================] - 81s 163ms/step - loss: 0.0327 - iou_score: 0.9599 - f1-score: 0.9732 - val_loss: 0.0295 - val_iou_score: 0.9400 - val_f1-score: 0.9657\nEpoch 4/5\n488/488 [==============================] - 79s 163ms/step - loss: 0.0321 - iou_score: 0.9451 - f1-score: 0.9681 - val_loss: 0.0494 - val_iou_score: 0.9390 - val_f1-score: 0.9650\nEpoch 5/5\n488/488 [==============================] - 79s 163ms/step - loss: 0.0284 - iou_score: 0.9420 - f1-score: 0.9670 - val_loss: 0.0211 - val_iou_score: 0.9400 - val_f1-score: 0.9657\n","metadata":{"id":"I1OFxBbnxC_I"}},{"cell_type":"markdown","source":"# Plots\n","metadata":{}},{"cell_type":"markdown","source":"**Using the values I pasted without transfer learning vs with transfer learning**","metadata":{}},{"cell_type":"code","source":"\nxpointsbefore = np.array([1,2,3,4,5])\nypointsbefore = np.array([0.1600,0.1888, 0.1967,0.2029,0.2063])\nypointsafter =np.array([0.5409,0.9487, 0.9599,0.9451,0.9420 ])\nplt.subplot(1, 3, 1)\nplt.plot(xpointsbefore,ypointsbefore)\n\nplt.subplot(1, 3, 2)\nplt.plot(xpointsbefore,ypointsafter)\nplt.subplot(1, 3, 3)\nplt.plot(xpointsbefore,ypointsafter,color='r', label='before')\n\nplt.plot(xpointsbefore,ypointsbefore,color='b', label='after')\nplt.legend()\nplt.xlim([0,1])\nplt.xlim([1,5])\n  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T00:16:15.260022Z","iopub.status.idle":"2022-08-21T00:16:15.262748Z","shell.execute_reply.started":"2022-08-21T00:16:15.262395Z","shell.execute_reply":"2022-08-21T00:16:15.262425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Table\nNote: Even though I added the first change of correcting the dataset , it has made no increase in my iou values which is why there is nothing to quantify\n\n\n![image.png](attachment:3b3d4471-9a6d-4599-8dd3-b31b128a5186.png)\n\n","metadata":{},"attachments":{"3b3d4471-9a6d-4599-8dd3-b31b128a5186.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"","metadata":{}}]}